# F分布 (F-Distribution)

#統計 

## 概要

F分布は、統計学および確率論で用いられる連続型確率分布であり、二つの独立なカイ二乗分布の比率として定義されます。主に分散の比較や分散分析（ANOVA）に用いられ、複数グループの平均値の差を検定する際の理論的基礎となります。

## 基本情報

**種別**: 連続分布

**確率密度関数**:
$$f(x) = \frac{1}{B(d_1/2, d_2/2)} \left(\frac{d_1}{d_2}\right)^{d_1/2} \frac{x^{d_1/2-1}}{(1 + \frac{d_1 x}{d_2})^{(d_1+d_2)/2}} \quad \text{for } x \geq 0$$

ここで：
- $x \geq 0$ に対して定義される
- $d_1$ と $d_2$ は正の整数で、それぞれ分子の自由度と分母の自由度を表す
- $B(a,b)$ はベータ関数: $B(a,b) = \frac{\Gamma(a)\Gamma(b)}{\Gamma(a+b)}$

## 数学的性質

**平均（期待値）**: 
$$E[F] = \frac{d_2}{d_2 - 2} \quad (d_2 > 2 \text{の場合})$$

**分散**: 
$$\text{Var}[F] = \frac{2d_2^2(d_1 + d_2 - 2)}{d_1(d_2 - 2)^2(d_2 - 4)} \quad (d_2 > 4 \text{の場合})$$

**最頻値（モード）**: 
$$\text{mode} = \frac{d_1 - 2}{d_1} \cdot \frac{d_2}{d_2 + 2} \quad (d_1 > 2 \text{の場合})$$

## 特徴と性質

### 非対称性
F分布は非対称で、右に長い裾を持つ分布です。

### 自由度による形状変化
- $d_1$（分子の自由度）が大きいほど分布は鋭くなる
- $d_2$（分母の自由度）が大きいほど裾が短くなる
- 両方の自由度が大きくなると正規分布に近づく

### 逆数の性質
$$\text{If } X \sim F_{d_1, d_2}, \text{ then } \frac{1}{X} \sim F_{d_2, d_1}$$

## 定義と構成

F分布は、独立にカイ二乗分布に従う2つの確率変数の比率として定義されます：

$$F_{d_1, d_2} = \frac{W_1/d_1}{W_2/d_2}$$

ここで：
- $W_1 \sim \chi^2_{d_1}$ （自由度 $d_1$ のカイ二乗分布）
- $W_2 \sim \chi^2_{d_2}$ （自由度 $d_2$ のカイ二乗分布）
- $W_1$ と $W_2$ は独立

## 他の分布との関係

### カイ二乗分布との関係
F分布はカイ二乗分布から構成されます：
$$F = \frac{\chi^2_{d_1}/d_1}{\chi^2_{d_2}/d_2}$$

### t分布との関係
自由度 $d_2$ のt分布の二乗は、自由度 $(1, d_2)$ のF分布に従います：
$$t_{d_2}^2 \sim F_{1, d_2}$$

### 正規分布との関係
大自由度の極限では正規分布に近似されます。

### ベータ分布との関係
$$\frac{d_1 F}{d_2 + d_1 F} \sim \text{Beta}\left(\frac{d_1}{2}, \frac{d_2}{2}\right)$$

## 特徴とモデリングへの応用

F分布の主要な応用は、分散分析 (ANOVA) です。これは、3つ以上のグループの平均を比較する手法であり、F分布を用いることで、各グループの平均の違いが偶然によるものかどうかを判定します。

### 主な応用分野

#### 1. 分散分析 (ANOVA: Analysis of Variance)

**一元配置分散分析**:
- 1つの要因による複数グループの平均値の比較
- F統計量: $F = \frac{\text{グループ間分散}}{\text{グループ内分散}} = \frac{MS_{between}}{MS_{within}}$
- 帰無仮説: 全グループの母平均が等しい ($H_0: \mu_1 = \mu_2 = \cdots = \mu_k$)

**二元配置分散分析**:
- 2つの要因とその交互作用の効果を検定
- 主効果と交互作用効果を個別に評価

**多元配置分散分析**:
- 3つ以上の要因を含む複雑な実験デザインの分析

#### 2. 分散の等質性検定
2つの母集団の分散が等しいかどうかを比較：
- $H_0: \sigma_1^2 = \sigma_2^2$ vs $H_1: \sigma_1^2 \neq \sigma_2^2$
- 検定統計量: $F = \frac{s_1^2}{s_2^2}$（より大きい分散を分子に）
- 詳細は[[F分布を用いた検定について]]を参照

#### 3. 回帰分析での応用

**モデルの有意性検定**:
- 回帰モデル全体の有意性を検定
- $F = \frac{MS_{regression}}{MS_{error}}$

**ネストモデルの比較**:
- より複雑なモデルが単純なモデルより有意に良いかを検定

#### 4. 実験計画法での応用
- 完全無作為化計画
- 乱塊法（randomized block design）
- ラテン方格法

### 実用的な応用例

#### マーケティング分野
- **A/B/Cテスト**: 3つ以上の広告戦略によるユーザー獲得数の違いの分析
- **価格戦略**: 異なる価格設定による売上効果の比較
- **地域別効果**: 複数地域でのキャンペーン効果の違いの評価

#### 医学・薬学研究
- **治療法の比較**: 異なる治療法による効果の違いの検定
- **薬剤投与量**: 複数の投与量による効果の比較
- **臨床試験**: 多群間での治療効果の評価

#### 農業・生物学
- **品種比較**: 異なる作物品種の収量比較
- **肥料効果**: 複数種類の肥料による生育効果の分析
- **環境条件**: 温度や湿度などの環境因子の影響評価

#### 工業・品質管理
- **製造条件**: 異なる製造条件による品質への影響
- **機械性能**: 複数の機械の性能比較
- **材料テスト**: 異なる材料の強度特性の比較

## 自由度の計算

### 一元配置分散分析の場合
- **分子の自由度** $d_1 = k - 1$ （$k$ はグループ数）
- **分母の自由度** $d_2 = N - k$ （$N$ は全サンプル数）

### 分散の等質性検定の場合
- **分子の自由度** $d_1 = n_1 - 1$
- **分母の自由度** $d_2 = n_2 - 1$

### 回帰分析の場合
- **分子の自由度** $d_1 = p$ （回帰係数の数）
- **分母の自由度** $d_2 = n - p - 1$ （$n$ はサンプル数）

## 数値例

### 例1: 一元配置分散分析
**問題**: 3つの教育方法（A、B、C）による学習効果の違いを検定
- グループA（n=5）: 平均 = 85, 分散 = 16
- グループB（n=5）: 平均 = 78, 分散 = 12  
- グループC（n=5）: 平均 = 82, 分散 = 20

**計算**:
- 全体平均: $\bar{x}_{total} = \frac{85×5 + 78×5 + 82×5}{15} = 81.67$
- 群間平方和: $SS_{between} = 5[(85-81.67)^2 + (78-81.67)^2 + (82-81.67)^2] = 122.3$
- 群内平方和: $SS_{within} = 4×16 + 4×12 + 4×20 = 192$
- $MS_{between} = 122.3/2 = 61.15$
- $MS_{within} = 192/12 = 16$
- $F = 61.15/16 = 3.82$
- 自由度: $(2, 12)$
- 臨界値（α=0.05）: $F_{0.05, 2, 12} = 3.89$
- 判定: $3.82 < 3.89$ なので帰無仮説受容（有意差なし）

### 例2: 分散の等質性検定
**問題**: 2つの製造ラインの品質のばらつきを比較
- ラインA: $n_1 = 10$, $s_1^2 = 25$
- ラインB: $n_2 = 12$, $s_2^2 = 16$

**計算**:
- $F = \frac{s_1^2}{s_2^2} = \frac{25}{16} = 1.56$（大きい方を分子に）
- 自由度: $(9, 11)$
- 臨界値（α=0.05, 両側）: $F_{0.025, 9, 11} = 3.59$
- 判定: $1.56 < 3.59$ なので帰無仮説受容（分散に有意差なし）

## 効果量と実用的意義

### η² (eta squared)
分散分析での効果量の指標：
$$\eta^2 = \frac{SS_{between}}{SS_{total}}$$

### 効果量の目安
- **小**: η² = 0.01
- **中**: η² = 0.06
- **大**: η² = 0.14

### 実用的意義
- F値が有意でも効果量が小さい場合は実用的意義が限定的
- サンプルサイズが大きいと小さな差でも統計的に有意になる
- 効果量と信頼区間を併せて報告することが重要

## 統計ソフトウェアでの実装

### R言語
```r
# 一元配置分散分析
result <- aov(response ~ group, data = mydata)
summary(result)

# F分布の確率計算
pf(f_value, df1, df2, lower.tail = FALSE)  # p値
qf(0.95, df1, df2)  # 臨界値
```

### Python
```python
from scipy import stats

# 一元配置分散分析
f_stat, p_value = stats.f_oneway(group1, group2, group3)

# F分布の確率計算
stats.f.cdf(f_value, dfn, dfd)  # 累積確率
stats.f.ppf(0.95, dfn, dfd)    # 臨界値
```

### SPSS
- Analyze → Compare Means → One-Way ANOVA
- Analyze → Compare Means → Independent-Samples T Test（2群の場合）

## 計算の詳細

### 平方和の分解
総平方和は以下のように分解されます：
$$SS_{total} = SS_{between} + SS_{within}$$

**群間平方和**:
$$SS_{between} = \sum_{i=1}^{k} n_i(\bar{x}_i - \bar{x})^2$$

**群内平方和**:
$$SS_{within} = \sum_{i=1}^{k} \sum_{j=1}^{n_i} (x_{ij} - \bar{x}_i)^2$$

### 平均平方の計算
$$MS_{between} = \frac{SS_{between}}{k-1}$$
$$MS_{within} = \frac{SS_{within}}{N-k}$$

## 二元配置分散分析

### モデル
$$x_{ijk} = \mu + \alpha_i + \beta_j + (\alpha\beta)_{ij} + \epsilon_{ijk}$$

ここで：
- $\alpha_i$: 第1要因の効果
- $\beta_j$: 第2要因の効果  
- $(\alpha\beta)_{ij}$: 交互作用効果
- $\epsilon_{ijk}$: 誤差項

### 検定される仮説
1. **主効果A**: $H_0: \alpha_1 = \alpha_2 = \cdots = 0$
2. **主効果B**: $H_0: \beta_1 = \beta_2 = \cdots = 0$
3. **交互作用**: $H_0: (\alpha\beta)_{ij} = 0$ for all $i,j$

## 関連項目

- [[カイ二乗分布]]: F分布の構成要素
- [[t分布]]: F分布との密接な関係（$t^2 \sim F_{1,\nu}$）
- [[02_正規分布]]: F検定の前提条件
- [[ベータ分布]]: F分布との変換関係
- [[03_確率分布間の関係性_特殊ケース]]: 分布間の関係について詳細解説
- [[04_参考文献]]: 詳細な参考文献

[戻る](00_確率分布一覧.md)
