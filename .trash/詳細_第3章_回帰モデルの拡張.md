# 第3章 回帰モデルの拡張：リッジ、Lasso、PLS

## 概要

この章では、多重共線性やスモールサンプル問題への対応として、線形回帰の拡張手法について詳細に論じている。従来の最小二乗法では対処困難な状況に対する実用的な解決策を提示している。

## リッジ回帰（Ridge Regression）

### 基本概念
リッジ回帰は**L2ノルムによる正則化**を通じて、回帰係数の推定を安定化する手法である。

### 特徴と効果
- **係数の分散を抑制**: 推定の安定性を確保
- **多重共線性への対応**: 係数が発散することを防ぐ
- **バイアス-バリアンス トレードオフ**: わずかなバイアスを許容して分散を大幅に削減

### 数学的定式化
目的関数に**L2ペナルティ項**を追加：
```
最小化: ||y - Xβ||² + λ||β||²
```
ここで、λは正則化パラメータ（リッジパラメータ）

## Lasso回帰（Lasso Regression）

### 基本概念
Lasso回帰は**L1ノルムの導入**により、一部の係数をゼロにする変数選択機能を持つ手法である。

### 特徴と効果
- **変数選択機能**: 重要でない変数の係数を自動的にゼロにする
- **スパースな解**: 解釈しやすいモデルを生成
- **特徴量の数がサンプル数より多い場合**にも適用可能

### 数学的定式化
目的関数に**L1ペナルティ項**を追加：
```
最小化: ||y - Xβ||² + λ||β||₁
```

## LARS（Least Angle Regression）

### 基本概念
LARSは**Lassoに関連する逐次的な変数選択アルゴリズム**である。

### 特徴
- **実装の簡易性**: アルゴリズムが比較的シンプル
- **パフォーマンスとの両立**: 計算効率と精度を両立
- **解の経路**: 正則化パラメータの変化に対する解の軌跡を効率的に計算

## PLS（部分最小二乗法）

### 基本概念と特徴
PLSは、**出力変数も考慮して次元削減を行う**点でPCR（主成分回帰）と根本的に異なる。

### PCRとの違い
| 手法 | 次元削減の基準 | 特徴 |
|------|---------------|------|
| PCR | 入力変数のみ | 出力との関係が捨象される可能性 |
| PLS | 入力と出力の両方 | 回帰精度を高める |

### PLS1の構造
PLS1は、**入力と出力の両方に共通する潜在変数を抽出**し、回帰精度を高める：

1. **T行列（潜在スコア）**を基盤とした新たな射影空間を構築
2. 入力と出力の共分散を最大化する方向を見つける
3. 逐次的に潜在変数を抽出

### 数学的構造
- PCAと類似の構造を持ちつつ、出力を考慮した修正を加える
- 射影行列の導出における係数計算が特徴的
- **条件数の改善効果**が期待できる

## スモールデータでの適用

### PLSの優位性
筆者は、スモールデータでの回帰においてPLSが有力な選択肢であると述べている：

- **サンプル数が少ない状況**での安定性
- **多重共線性**への耐性
- **予測精度**と**解釈可能性**のバランス

### 手法の使い分け
各手法の特性を理解した上での適切な選択が重要：

- **リッジ回帰**: 全ての変数を保持したい場合
- **Lasso回帰**: 変数選択が必要な場合  
- **PLS**: 予測精度を重視し、かつ解釈可能性も求める場合

## まとめ

スモールデータにおける回帰問題では、従来の最小二乗法の限界を補う様々な手法が利用できる。リッジ回帰、Lasso回帰、LARS、PLSはそれぞれ異なる特徴を持ち、問題の性質に応じて適切に選択する必要がある。特にPLSは、出力変数を考慮した次元削減により、スモールデータでの回帰において優れた性能を示すことが期待される。

---

戻る: [[章ごとのまとめ]]