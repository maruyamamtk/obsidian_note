---
kindle-sync:
  bookId: '30350'
  title: 実践的データ基盤への処方箋〜 ビジネス価値創出のためのデータ・システム・ヒトのノウハウ
  author: ゆずたそ、渡部 徹太郎、伊藤 徹郎
  asin: B09MSX9MQV
  lastAnnotatedDate: '2024-03-09'
  bookImageUrl: 'https://m.media-amazon.com/images/I/81B-0YN8YuL._SY160.jpg'
  highlightsCount: 69
---
# 実践的データ基盤への処方箋〜 ビジネス価値創出のためのデータ・システム・ヒトのノウハウ
## Metadata
* Author: [ゆずたそ、渡部 徹太郎、伊藤 徹郎](https://www.amazon.comundefined)
* ASIN: B09MSX9MQV
* Reference: https://www.amazon.com/dp/B09MSX9MQV
* [Kindle link](kindle://book?action=open&asin=B09MSX9MQV)

## Highlights
データの品質はデータソースで担保しましょ — location: [587](kindle://book?action=open&asin=B09MSX9MQV&location=587) ^ref-3737

---
アプリケーションとストレージを区別していることに注意してください。紙のメモ帳のようにアナログな媒体では、アプリケーションとストレージは同じことが多いですが、デジタルな媒体では区別されます。例えば、スマートフォンの「メモ帳アプリ」はヒトが操作するアプリケーションです。一方で、メモの中身（データ）が実際に保存されているのは、スマートフォン端末のストレージだったり、クラウドストレージと呼ばれるサーバです。 — location: [659](kindle://book?action=open&asin=B09MSX9MQV&location=659) ^ref-29177

---
ちなみに「いかにアナログな業務をデジタル化するか」というのは多くの企業が抱えている課題で、根本的にはこの問題と向き合うことになると筆者は考えています。営業スタッフが商談のメモを手帳に書いているようではデータ — location: [722](kindle://book?action=open&asin=B09MSX9MQV&location=722) ^ref-3564

---
活用することはできません。 — location: [725](kindle://book?action=open&asin=B09MSX9MQV&location=725) ^ref-28815

---
データソースからデータレイクへとデータを転送する方法についても、第2章を参照してください。主にETL（Extract：データ抽出、Transform：データ整形、Load：データ出力）ツールと呼ばれるソフトウェアを活用することが多いです。 — location: [810](kindle://book?action=open&asin=B09MSX9MQV&location=810) ^ref-58139

---
2章で解説しますが、データウェアハウス層のデータは、分析用DB（特にデータウェアハウス製品と呼ばれるもの）で集計・管理することが望ましいです。ExcelやGoogleSpreadsheetのような表計算ソフト、TableauやRedashのようなBIツール（Business Intelligence Tool：ビジネスの意思決定に寄与するデータ分析ソフトウェア）ではなく、専用のデータベースで共通指標を管理しましょう。 — location: [859](kindle://book?action=open&asin=B09MSX9MQV&location=859) ^ref-37562

---
自部門の視点で「おそらくこれが売上だろう」と想像して定義すると、他部門の「売上」と乖離が発生します。経理部門の立場では、消費者から預かった消費税を税務署に申告・納付しないといけないので「売上」と「消費税」を分ける必要があります。一方で、Webサイト運営部門の立場では、Webサイトの利用者が「支払い総額」を過不足なく決済できることが大事なので「消費税込みの合計金額」に注目して仕事をしています。 — location: [873](kindle://book?action=open&asin=B09MSX9MQV&location=873) ^ref-45313

---
「売上」などの主要な共通指標は、各部門が表計算ソフトやBIツールで集計するのではなく、部門横断で分析用DBに「売上」データを用意してください。 — location: [900](kindle://book?action=open&asin=B09MSX9MQV&location=900) ^ref-44000

---
1-6 — location: [933](kindle://book?action=open&asin=B09MSX9MQV&location=933) ^ref-9364

---
節の繰り返しになりますが、データレイク層 — location: [933](kindle://book?action=open&asin=B09MSX9MQV&location=933) ^ref-3937

---
でのデータ修正は推奨しません。データウェアハウス層でクレンジングを行います。 — location: [934](kindle://book?action=open&asin=B09MSX9MQV&location=934) ^ref-59061

---
本書ではデータの流れの順番で説明していますが、データウェアハウス層の設計に着手するのは、データマート層が使われるようになったあとにしましょう（図1-11）。「 — location: [971](kindle://book?action=open&asin=B09MSX9MQV&location=971) ^ref-45505

---
具体的には「データマート — location: [1020](kindle://book?action=open&asin=B09MSX9MQV&location=1020) ^ref-10954

---
が多すぎる」「ツールが分断される」といった2つの問題が起きます。それぞれ解説します。 — location: [1020](kindle://book?action=open&asin=B09MSX9MQV&location=1020) ^ref-64013

---
BIツールや表計算ソフトがデータマートの役割を果たすことがあります。前述の例では「週次の◯◯ジャンルの売上」という指標を分析用DBではなく、BIツールや表計算ソフトで集計するケースです。 — location: [1030](kindle://book?action=open&asin=B09MSX9MQV&location=1030) ^ref-29762

---
● 事業目標にそぐわない課題を解く ● 優先順位が低い施策を進める ● データ利用の5W1Hを想定せずに現場に押し付ける ● 一度のリリースに全力をかける — location: [1119](kindle://book?action=open&asin=B09MSX9MQV&location=1119) ^ref-50897

---
この商品データは、ETLツールを使うことで、データソースからデータレイク層へとコピーできます。 — location: [1162](kindle://book?action=open&asin=B09MSX9MQV&location=1162) ^ref-50566

---
ところが、データ整備に力を注ごうとした企業の中には、データ生成者の存在を無視して「メタデータのための専用ツールや専門部隊」を導入するケースが見受けられます。 — location: [1202](kindle://book?action=open&asin=B09MSX9MQV&location=1202) ^ref-39331

---
しかし、データソースに一番詳しく、内容に責任を持っているのは、そのデータを生成する人たちです。ビジネスや製品の変化によるデータソースの変化を、誰よりも早く正確に検知できるのも、データを生成する人たちです。 — location: [1214](kindle://book?action=open&asin=B09MSX9MQV&location=1214) ^ref-31531

---
そこで、50％ルールを設けましょう。問い合わせ対応などの運用作業を50％以下に抑えて、改善活動に50％以上の時間を費やすというルールです。 — location: [1373](kindle://book?action=open&asin=B09MSX9MQV&location=1373) ^ref-5970

---
ファイルは最も慣れ親しんだデータの表現形式の1つでしょう。実際に、データ収集の対象の多くがファイルです。 — location: [1590](kindle://book?action=open&asin=B09MSX9MQV&location=1590) ^ref-38936

---
製品例としては、Amazon S3 のイベント通知 注5 や、Google Cloud Storage トリガー 注6 が挙げられます。 — location: [1606](kindle://book?action=open&asin=B09MSX9MQV&location=1606) ^ref-50340

---
例えば、配置が完了したら、特定のディレクトリの中に「20210725_000020」のような配置完了日時をファイル名に付けたファイルを配置することで、トリガファイルを実現できます。 — location: [1612](kindle://book?action=open&asin=B09MSX9MQV&location=1612) ^ref-9884

---
そこで図2-12のような カーソル を用いて取得する方法をとります。 — location: [1750](kindle://book?action=open&asin=B09MSX9MQV&location=1750) ^ref-1124

---
このように、収集対象のデータが追記のみの場合と更新もある場合とでは収集方法は変わるのですが、どちらの方法でも注意すべき点があります。それは インデックス を利用することです。 — location: [1782](kindle://book?action=open&asin=B09MSX9MQV&location=1782) ^ref-34906

---
レプリカから収集する方法では、複製したデータベースの管理者はデータソースのシステム管理者です。 — location: [1939](kindle://book?action=open&asin=B09MSX9MQV&location=1939) ^ref-40890

---
一方で、更新ログ収集の場合は、複製したデータベースはデータ基盤側の管理となるでしょう。 — location: [1943](kindle://book?action=open&asin=B09MSX9MQV&location=1943) ^ref-37900

---
更新ログ収集はCDCツールが本命 — location: [1944](kindle://book?action=open&asin=B09MSX9MQV&location=1944) ^ref-1116

---
一方で、CDCツールは前述した更新ログ収集のしくみよりもさらに複雑になるデメリットがあります。というのも、このしくみを自前でつくることは難しく、専用ツールを使う必要があることに加えて、利用できるツールは高価です。具体的には、AWSのDatabase Migration Service、GCPのDatastream、Oracle社のGoldenGate、Qlik（Attunity）、troccoなどがあります。 — location: [1956](kindle://book?action=open&asin=B09MSX9MQV&location=1956) ^ref-15967

---
つまり、利用する側としては「順序性保証あり」「メッセージの重複なし」「可視性タイムアウトが長い」という状態が理想 — location: [2157](kindle://book?action=open&asin=B09MSX9MQV&location=2157) ^ref-27693

---
分散メッセージキュー — location: [2180](kindle://book?action=open&asin=B09MSX9MQV&location=2180) ^ref-18355

---
GCPにはCloud Pub/Subというサービスがあります。 — location: [2183](kindle://book?action=open&asin=B09MSX9MQV&location=2183) ^ref-15729

---
GCPではCloud FunctionsがAWSのLambdaと同様のサービスであり、Cloud Dataflow TemplateがKinesis Data Firehoseと同等のサービスです。 — location: [2188](kindle://book?action=open&asin=B09MSX9MQV&location=2188) ^ref-40540

---
GCPであればCloud Dataflowがその機能です。 — location: [2194](kindle://book?action=open&asin=B09MSX9MQV&location=2194) ^ref-63262

---
ETLはExtract Transform Loadの略であり、データを「抽出」「加工」「ロード」することを意味します。 — location: [2197](kindle://book?action=open&asin=B09MSX9MQV&location=2197) ^ref-12780

---
クラウド上のサービスではAWSのGlueやDMS（Database Migration Service）、GCPのCloud Data Fusion、DMS（Database Migration Service）が有名でしょう（ — location: [2204](kindle://book?action=open&asin=B09MSX9MQV&location=2204) ^ref-40049

---
データレイクには収集したデータを加工せずにそのまま格納するのが望ましいです。 — location: [2263](kindle://book?action=open&asin=B09MSX9MQV&location=2263) ^ref-14625

---
加工せずに保存することが好ましい理由は、加工を試みると失敗してデータを損失する可能性があるためです。 — location: [2267](kindle://book?action=open&asin=B09MSX9MQV&location=2267) ^ref-57518

---
そのとき、クラウドが利用できる場合はオブジェクトストレージをデータレイクに利用します。 — location: [2293](kindle://book?action=open&asin=B09MSX9MQV&location=2293) ^ref-17913

---
AWSのAmazon S3 注23 やGCPのCloud Storage 注24 です。 — location: [2298](kindle://book?action=open&asin=B09MSX9MQV&location=2298) ^ref-60128

---
CSVやJSON形式のデータの場合は、オブジェクトストレージに蓄積する以外に、データウェアハウスで利用する分析用DBに直接入れてしまう選択肢があります。 — location: [2317](kindle://book?action=open&asin=B09MSX9MQV&location=2317) ^ref-57977

---
2015年頃はデータレイクにはオブジェクトストレージを利用するのが世の中の主流でしたが、執筆時点の2021年ではデータレイクにはデータウェアハウスと同じ分析用DBを利用する方式が流行っているように感じます。データレイクとデータウェアハウスを同じ分析用DBで扱うことにより管理する対象が1つで済むという点と、SQLを用いてデータウェアハウス生成処理が記述できて開発しやすいという点がその背景にあると考えています。 — location: [2336](kindle://book?action=open&asin=B09MSX9MQV&location=2336) ^ref-40709

---
Redshiftはデータ分析でよく行われるデータの抽出や集計といった機能を効率よく処理するために特化しています — location: [2372](kindle://book?action=open&asin=B09MSX9MQV&location=2372) ^ref-26879

---
世の中にはオペレーショナルDBと分析用DBの2種類があります。AuroraはオペレーショナルDBで、Redshiftは分析用DBに分類されます。 — location: [2375](kindle://book?action=open&asin=B09MSX9MQV&location=2375) ^ref-63216

---
世の中で単に「データベース」といった場合は基本的にオペレーショナルDBを指します。 — location: [2384](kindle://book?action=open&asin=B09MSX9MQV&location=2384) ^ref-54076

---
一方で、列の値の平均をとったり合計をしたりといった集計の操作は苦手です。 — location: [2393](kindle://book?action=open&asin=B09MSX9MQV&location=2393) ^ref-913

---
AWSのAmazon Redshift、GCPのBigQuery、そしてSnowflakeなどがあります。 — location: [2401](kindle://book?action=open&asin=B09MSX9MQV&location=2401) ^ref-46009

---
分析用DBは応答速度よりも単位時間あたりの処理速度である「スループット」を重視します。 — location: [2402](kindle://book?action=open&asin=B09MSX9MQV&location=2402) ^ref-57436

---
執筆時点で主流の分析用DBは提供形態とHadoopベースかどうかで大きく4つに分類できます。 — location: [2431](kindle://book?action=open&asin=B09MSX9MQV&location=2431) ^ref-53494

---
Hadoopは本来さまざまな分散処理をするためのソフトウェア群であり、データ分析はその中の機能の一部でしかありませんでした。そのため、アーキテクチャが複雑になりがちで、データのパーティションが多いと遅くなることや複雑な結合に弱いといった欠点がありました。 — location: [2461](kindle://book?action=open&asin=B09MSX9MQV&location=2461) ^ref-9400

---
データソースがあるクラウドで利用できる分析用DBを選ぶのが自然な選択 — location: [2496](kindle://book?action=open&asin=B09MSX9MQV&location=2496) ^ref-41095

---
オペレーショナルDBであればUPDATE文が正しいのですが、分析用DBは一度テーブルをDROPしてから再作成するのが正解です。 — location: [2553](kindle://book?action=open&asin=B09MSX9MQV&location=2553) ^ref-48531

---
テーブルの一部を更新する場合は、UPDATEやDELETEではなく、更新した内容を持つ新たなテーブルを用意し、 — location: [2594](kindle://book?action=open&asin=B09MSX9MQV&location=2594) ^ref-25060

---
データを入れるときは、1件づつINSERTするのではなく、複数件まとめてロードする： — location: [2599](kindle://book?action=open&asin=B09MSX9MQV&location=2599) ^ref-44429

---
データ基盤「専用」のワークフローエンジンにするか、会社のワークフローエンジンに「相乗り」にするか — location: [2661](kindle://book?action=open&asin=B09MSX9MQV&location=2661) ^ref-6765

---
具体的にどの製品がよいかを解説すると、執筆時点の2021年ではApache Airflow 注30 がもっとも有力な候補と言えます。 — location: [2707](kindle://book?action=open&asin=B09MSX9MQV&location=2707) ^ref-4448

---
レベル1：データ活用の初期段階で、属人的にデータが活用されている ● レベル2：データ活用プロセスに最低限の統制がとられ、再現可能である ● レベル3：データ活用における基準を設け、それが守られている ● レベル4：プロセスを数値化し、モニタリング・管理できている ● レベル5：プロセス改善のゴールを数値化し、それに向けた最適化に取り組んでいる — location: [2826](kindle://book?action=open&asin=B09MSX9MQV&location=2826) ^ref-9295

---
ビジョンと基本理念を混同してしまうケースもありますが、ビジョンというのは — location: [3066](kindle://book?action=open&asin=B09MSX9MQV&location=3066) ^ref-46063

---
組織がありたい状態目標のことで、基本理念はデータを扱う際の行動原則や指針のような位置付けと解釈してかまいません。 — location: [3067](kindle://book?action=open&asin=B09MSX9MQV&location=3067) ^ref-22029

---
KPT（Keep, Problem, Try）やYWT（やったこと、わかったこと、つぎにやること）方式のふりかえりはよく使われるフレームワークです。 — location: [3187](kindle://book?action=open&asin=B09MSX9MQV&location=3187) ^ref-2006

---
多くの企業では、このような責務を担っている部署はデータ活用組織ではなく、セキュリティ専任の部門が担っていることが多いでしょう。 — location: [3267](kindle://book?action=open&asin=B09MSX9MQV&location=3267) ^ref-15866

---
データ活用に柔軟に対応しようとすると、さまざまな例外パターンが生まれ、やがて管理が煩雑になりセキュリティ水準を守れなくなります。 — location: [3269](kindle://book?action=open&asin=B09MSX9MQV&location=3269) ^ref-24286

---
こうした環境での権限管理には IAM（Cloud Identity and Access Management） を用います。 — location: [3320](kindle://book?action=open&asin=B09MSX9MQV&location=3320) ^ref-14256

---
セキュリティポリシーはシンプルに保ち続けることが重要です。 — location: [3331](kindle://book?action=open&asin=B09MSX9MQV&location=3331) ^ref-7039

---
そうした懸念に備えて、いくつかのグループに分けて管理することで、一定の水準を保つことができます。 — location: [3333](kindle://book?action=open&asin=B09MSX9MQV&location=3333) ^ref-58106

---
また、棚卸の際に見落としがちなものとして、さまざまな処理を実行するために利用しているシステムのアカウントキーが挙げられます。セキュリティ面も考慮して、一定期間を経過した場合に失効し、新しいキーに洗い替えする方法が適切でしょう。 — location: [3417](kindle://book?action=open&asin=B09MSX9MQV&location=3417) ^ref-13488

---
ここで筆者が推奨するのは、アーカイブフォルダなどをあらかじめ準備しておき、棚卸対象となったリソースを一括でそちらに移動し、ユーザ側には表示しないようにする運用です。 — location: [3437](kindle://book?action=open&asin=B09MSX9MQV&location=3437) ^ref-11116

---
個人情報を匿名化する上で、 K-匿名性 という重要な考え方があります。ある特定の個人情報データを集約した際、その特定性が何人までの粒度なのかという考え方です。このKに入る数字がその最小特定可能人数です。ポリシーで10人までの特定性を定義した場合は10-匿名性となり、保有しているデータを集約しても、個人の特定確率は1/10です。 — location: [3485](kindle://book?action=open&asin=B09MSX9MQV&location=3485) ^ref-57717

---
脅威はシステムや組織に危害を与える潜在的な要因のことを指します。脆弱性は脅威によって発生する内在的な弱さのことです。また、それによってどんな損害を被りうるのかがリスクの考え方になります。 — location: [3524](kindle://book?action=open&asin=B09MSX9MQV&location=3524) ^ref-1856

---
