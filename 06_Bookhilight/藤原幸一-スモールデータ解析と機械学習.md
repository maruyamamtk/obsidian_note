---
kindle-sync:
  bookId: '18385'
  title: スモールデータ解析と機械学習
  author: 藤原幸一
  asin: B09SPGL2ZH
  lastAnnotatedDate: '2025-06-02'
  bookImageUrl: 'https://m.media-amazon.com/images/I/918q-hiwa9L._SY160.jpg'
  highlightsCount: 48
---
# スモールデータ解析と機械学習
## Metadata
* Author: [藤原幸一](https://www.amazon.comundefined)
* ASIN: B09SPGL2ZH
* Reference: https://www.amazon.com/dp/B09SPGL2ZH
* [Kindle link](kindle://book?action=open&asin=B09SPGL2ZH)

## Highlights
● X はY に先行して発生しなければならない． ● Y はX が発生していないときは， 発生してはならない． ● Y はX が発生したら， 必ず発生しなければならない． これら三条件の一つでも成り立っていない場合は， 擬似相関であることが多 いです． — location: [14419](kindle://book?action=open&asin=B09SPGL2ZH&location=14419) ^ref-57793

---
半正定値行列， または非負定値行列とは， 任意のベクトル x について， 常に二次形式が xAx ≥ 0 と なる行列A のことです． — location: [17479](kindle://book?action=open&asin=B09SPGL2ZH&location=17479) ^ref-12160

---
第1主成分の導出についての議論をまとめると ●第1主成分のローディングベクトルp1 は， 主成分得点t1 の分散s2 t1 を最 大とするように決定します． ● p1 を求める問題は， データ行列X の共分散行列V の固有値問題に帰着 します． ● p1 はV の最大固有値に対応する固有ベクトルになります． — location: [18351](kindle://book?action=open&asin=B09SPGL2ZH&location=18351) ^ref-33376

---
また， 各変数の分散をあらかじめ1に標準化している場合には， 元の変数 が持つ分散よりも大きな分散を持つ主成分のみを採用するという考え方か ら， 1以上の固有値に対応する主成分を採用するという基準が利用されるこ ともありますが*28， これにもやはり根拠はありません． — location: [24031](kindle://book?action=open&asin=B09SPGL2ZH&location=24031) ^ref-36398

---
U とV は直交行列 で， 直交行列は写像として回転を表します． Σは対角行列であり， 対角行列 はv のそれぞれの要素を対応する対角成分で拡大・縮小させる作用がありま す． — location: [25342](kindle://book?action=open&asin=B09SPGL2ZH&location=25342) ^ref-21943

---
固有値分解は正方行列に対してのみ適用できるのに対して， 特異値分解は 任意の形の行列に適用できます． — location: [25342](kindle://book?action=open&asin=B09SPGL2ZH&location=25342) ^ref-49182

---
PCAを導出する際に 登場した共分散行列V は半正定値行列でしたので， V を固有値分解した結 果とデータ行列X をSVDした結果は， 一致することになります． — location: [25343](kindle://book?action=open&asin=B09SPGL2ZH&location=25343) ^ref-44666

---
実際に固有値問題と SVD のどちらを使うかについては， データ行列 X ∈ RN×M のサイズを考慮して決めることをおすすめします． — location: [25779](kindle://book?action=open&asin=B09SPGL2ZH&location=25779) ^ref-42748

---
●仮定1： E[ε] = 0 （誤差の平均は0） ●仮定2： Cov[ε] = σ2I （誤差は等分散・無相関）*5 を満たすとき， 最小二乗法で求めた回帰係数（最小二乗推定量） は最良線形 不偏推定量 （Best Linear Unbiased Estimator; BLUE） になる， というも のです． — location: [30147](kindle://book?action=open&asin=B09SPGL2ZH&location=30147) ^ref-49590

---
行列 A の条件数κ(A) は， A の最大特異値，最小特異値を σmax， σmin として κ(A) = σmax/σmin で定義されます． — location: [32769](kindle://book?action=open&asin=B09SPGL2ZH&location=32769) ^ref-53842

---
本 来，最小二乗解はBLUEであり， 回帰係数のばらつきは最も小さくなるはず です． しかし， 多重共線性の問題が発生すると， まったく信頼できない結果 となってしまうのです． — location: [34517](kindle://book?action=open&asin=B09SPGL2ZH&location=34517) ^ref-15719

---
スモールデータを扱う場合， N<M は一般的にあり得る状況であり， こ の場合は最小二乗法そのものが適用できません． — location: [35391](kindle://book?action=open&asin=B09SPGL2ZH&location=35391) ^ref-54070

---
似た働きをする都合のよい行列があれば便利です． 行列Aに対して ● AA+A = A ● A+AA+ = A+ ● (AA+) = AA+ ● (A+A) = A+A を満たす行列A+ がただ一つ存在して， これを擬似逆行列とよびます*18． — location: [35391](kindle://book?action=open&asin=B09SPGL2ZH&location=35391) ^ref-10427

---
この中で最小ノルム解x∗ とは， 解xの中で最もノルムxが小さくなる 解のことで， 擬似逆行列A+ によって求めた解は最小ノルム解x∗ となるの です． — location: [36265](kindle://book?action=open&asin=B09SPGL2ZH&location=36265) ^ref-41633

---
第R主成分に対応する特異値をσR としましょう． このときのT  R TR の条 件数はκ(T  R TR) = σ2 max/σ2 R です． 元のデータの積和行列XX の条件数 がκ(XX) = σ2 max/σ2 min であったため， 前処理としてPCAを適用するこ とで， 条件数がκ(T  R TR)/κ(XX) = σ2 min/σ2 R倍に改善することになり， — location: [37575](kindle://book?action=open&asin=B09SPGL2ZH&location=37575) ^ref-61435

---
なお， すべての主成分を採用する場合， つまりR = M だとすると最小特 異値はσmin のままなので， 条件数は改善しません． したがって， PCRにて 回帰係数を推定する場合は， 適切に次元を削減する必要があります． — location: [38012](kindle://book?action=open&asin=B09SPGL2ZH&location=38012) ^ref-18286

---
最小固有値が大きいということは， 同時に最小特異値が大きいというこ とを意味するので*27， 逆行列計算の条件のよさを測る条件数κ(A)を小さく することになります*28． — location: [39324](kindle://book?action=open&asin=B09SPGL2ZH&location=39324) ^ref-42200

---
PLS では， 共通の潜在変数T を用いて入力データ X と出力データ y を表現します． PCR との違いは， 潜在変数T の決定には入力のみならず， 出力の情報も活用するところにあり ます — location: [40633](kindle://book?action=open&asin=B09SPGL2ZH&location=40633) ^ref-63770

---
PLSの特徴は， 入力データX と出力データy を共通の潜在変数T で表現 するところです． PLSの構造を図3.8に示します． 出力変数が一つのときのPLSを， PLS1とよびます． PLS1モデルは X = T P  + E =  R r=1 trp r + E (3.95) y = T d + f =  R r=1 drtr + f (3.96) と表現されます． — location: [40634](kindle://book?action=open&asin=B09SPGL2ZH&location=40634) ^ref-44025

---
しかし， 3.3節で説明したように， 回帰分析では入出力間の相関関係が 重要です． したがって， データの次元を削減する際にX のみしか考慮しな いPCRには， 不満が残ります． 回帰モデルを学習させるのであれば， 入力 データX を次元削減する際に出力データy も考慮した方が， より回帰モデ ルに適した次元削減が可能ではないか， と考えられるのです． — location: [41071](kindle://book?action=open&asin=B09SPGL2ZH&location=41071) ^ref-31849

---
3.12 （中央） のような状況も想定されるためです． この図は横軸が真値， 縦軸が予測値であり， 対角線にサンプルが乗るほどよい予測をしているこ とを意味します． 図3.12 （中央） の場合， モデルはほぼ一定値を出力して おり， ほとんど予測をしていないことになりますが， — location: [50246](kindle://book?action=open&asin=B09SPGL2ZH&location=50246) ^ref-32958

---
AICは 「モデルのデータとの適合度と， モデルの複雑さのバランス」 を評価 するもので AIC = −2 ln L + 2k (4.1) で与えられ， AIC を最小とするパラメータを選択します． — location: [55926](kindle://book?action=open&asin=B09SPGL2ZH&location=55926) ^ref-13105

---
ベイズ情報量規準 （Bayesian Information Criterion; BIC） があります． BICは AIC = −2 ln L + 2k ln N (4.11) で定義されます． — location: [57237](kindle://book?action=open&asin=B09SPGL2ZH&location=57237) ^ref-22723

---
ここでFin は変数選択におけるF値の閾値ですが， Fin = 2と することが多いようです． 2.6節で説明したように， 分散はその変数の持つ — location: [57674](kindle://book?action=open&asin=B09SPGL2ZH&location=57674) ^ref-14647

---
情報量の大きさですから， Fin = 2というのは誤差の情報量よりも出力の推 定値の情報量が倍はあって欲しい， ということを意味しています． — location: [58110](kindle://book?action=open&asin=B09SPGL2ZH&location=58110) ^ref-17409

---
リッジ回帰は， 最小二乗法における多重共線性の問題を回避するのが目的 でした． 一方で， Lasso回帰では， 特定の回帰係数が強制的に0に設定され — location: [58548](kindle://book?action=open&asin=B09SPGL2ZH&location=58548) ^ref-6533

---
るため， 入力変数の選択につながります． — location: [58983](kindle://book?action=open&asin=B09SPGL2ZH&location=58983) ^ref-52555

---
LARS は， 線形回帰モデルにおける学習用の出力データ y とその推定値ηˆ との残 差*13ε = y − ηˆと相関が大きい変数を， 入力変数として一つずつ追加してい く方法です [15]． — location: [60731](kindle://book?action=open&asin=B09SPGL2ZH&location=60731) ^ref-46683

---
ここでは 「誤差」 ではなく 「残差」 になっています． 通常はこれらの使い分けはあまり意識することは ありませんが， ここでは明確に 「残差」 としました． 誤差は求めようとする真のモデルから算出される 値と測定値との差を表しますが， 真のモデルはあくまでも理論的な理想であるため， 誤差はデータから 直接計算することはできません． 一方，「残差」 は実データを用いて推定されたモデルから算出される推 定値と測定値との差のことです． つまり， 誤差とは異なり残差はデータから求めることができます． こ こでは， LARS アルゴリズムの中で登場し実際に計算できる値なので， 残差と明示しました． — location: [60732](kindle://book?action=open&asin=B09SPGL2ZH&location=60732) ^ref-44737

---
NC法を用いることで， 類似の相関関係に従うサンプルのペアを検出でき ます． NC法の結果から相関関係を指標とした類似度行列を構築し， SCでク ラスタリングを実行するのがNCSCです． — location: [67723](kindle://book?action=open&asin=B09SPGL2ZH&location=67723) ^ref-48043

---
このようにLDAとは， クラス識別に資する 射影軸w をデータから学習する手法であり， PCA同様に次元削減手法とみ なせます． — location: [75149](kindle://book?action=open&asin=B09SPGL2ZH&location=75149) ^ref-38010

---
まとめると， LDAでは二つの条件 1. 射影後の二つのクラスがなるべく離れるように， クラス間平均の差がな るべく大きくなるようにする． 2. 射影後のクラス内のデータがなるべく密集するように， クラス内変動が なるべく小さくなるようにする． を同時を達成できるwを見つけるということになります． — location: [76023](kindle://book?action=open&asin=B09SPGL2ZH&location=76023) ^ref-63771

---
J(w) = wSBw wSW w — location: [76460](kindle://book?action=open&asin=B09SPGL2ZH&location=76460) ^ref-7827

---
つまり， J(w) の最大値は行列S−1 W SB の最大固有値であり， 求める射影軸w は最大固有値に対応する固有ベクトルであることがわかり ます ． — location: [77333](kindle://book?action=open&asin=B09SPGL2ZH&location=77333) ^ref-11213

---
n∈C1  xnx n +  n∈C2 xnx n  w — location: [78644](kindle://book?action=open&asin=B09SPGL2ZH&location=78644) ^ref-41135

カッコの位置がおかしいかも

---
このことから， 第3章で説明した最小二乗法の性質は， LDAも同様に有 していることになります． つまり， 入力変数間に相関関係があると， 最小二 乗法同様に多重共線性の問題が発生し， 学習させたモデルが安定しなくなり ます ． — location: [78645](kindle://book?action=open&asin=B09SPGL2ZH&location=78645) ^ref-59551

---
そこで， 検査結果（分類結果） に与える検査手法（分類モデル） 自体の性 能とカットオフ決定の問題を切り離して考えることにします． このときに使 われるのがROC曲線です*9． — location: [79956](kindle://book?action=open&asin=B09SPGL2ZH&location=79956) ^ref-27735

---
それぞれのクラスのサンプルの数が均衡している通常のデータの解析に も， アンサンブル学習は広く使われていますが， 不均衡データの解析にも使 われることがあります． これは通常の機械学習の方法よりもアンサンブル学 習の方が， 不均衡データでも比較的よい性能を達成しやすいとされているた めです． — location: [85197](kindle://book?action=open&asin=B09SPGL2ZH&location=85197) ^ref-19965

---
このようにブースティングでは， 以前の弱分類器の学習 結果を取り込むため， 完全にそれぞれの弱分類器の学習が独立しているバギ ングと比較して， 高い性能が得やすいといわれています． — location: [87819](kindle://book?action=open&asin=B09SPGL2ZH&location=87819) ^ref-3970

---
LOFk(xi) = xj∈Nk(xi) LRD(xj) |Nk(xi)|/ LRD(xi) (6.3) — location: [96557](kindle://book?action=open&asin=B09SPGL2ZH&location=96557) ^ref-46030

---
このスコアは， xi の近傍サンプルの局所到達可能性密度の 平均を， xi の自身の局所到達可能性密度で割ったものです． したがって， LOFk(xi)が1に近いとき， xi の局所密度はその近傍と同程度なので， 正常 であるとみなされます． 一方で， LOFk(xi)が1を大きく上回るとき， 異常 であると判定されます． — location: [96557](kindle://book?action=open&asin=B09SPGL2ZH&location=96557) ^ref-45563

---
iForestでは， まず正常データから RF同様に複数の判別木を学習させま す． 正常データは判別木のルートから遠い枝の部分で分類されると期待され ます． — location: [96994](kindle://book?action=open&asin=B09SPGL2ZH&location=96994) ^ref-34951

---
*7 少しややこしいですが， c(n) はありとあらゆる判別木の構造を想定したときの理論的に計算される h(x) の平均で， 学習サンプルの数n だけで決まります． それに対して， E(h(x)) は実際の学習結果か ら求めた h(x) の平均です． — location: [97433](kindle://book?action=open&asin=B09SPGL2ZH&location=97433) ^ref-9478

---
つまり， T2統計量が小さいことは， 監視対象のサン プルがデータの平均に近いということを意味します． — location: [99616](kindle://book?action=open&asin=B09SPGL2ZH&location=99616) ^ref-25999

---
Q統計量は変数間の相関関係を尺度とした正常データとの非類 似度といえます． つまり， Q統計量が小さいほどxは正常に近いと判断でき ます — location: [99616](kindle://book?action=open&asin=B09SPGL2ZH&location=99616) ^ref-32622

---
実際に， 付録 A.4節で説明するように， 隠れ層の活性化関数として恒等関数を選ぶと*19， AEはPCAと一致することが知られています． つまり， AEはPCAに非線 形性を取り入れた拡張であると考えることができます． — location: [102675](kindle://book?action=open&asin=B09SPGL2ZH&location=102675) ^ref-47187

---
Contaminationは6.5節で説明した信頼区 間に相当するパラメータで， これらはLOFの性能をTEプロセスで評価し た文献 [61]を参考に決定しました． — location: [107917](kindle://book?action=open&asin=B09SPGL2ZH&location=107917) ^ref-48708

---
1. どのような患者に （Patient） 2. どのような介入をしたら （Intervention） *5 3. 何と比較して （Comparison） 4. どのような結果になるか （Outcome） — location: [114908](kindle://book?action=open&asin=B09SPGL2ZH&location=114908) ^ref-36553

---
